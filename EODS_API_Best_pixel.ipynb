{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Notebook to work correctly, the virtual memory on the Windows 10 machine where this Notebook is being run must be increased. This is because the notebook will download and process satellite images that are, generally, very large. In order to achieve this, there are some system modifications that need to be done before running the Notebook. To increase the virtual memory of the machine:\n",
    "\n",
    "Go to Control panel---> System---> Advanced System Settings.\n",
    "\n",
    "In the window that appears select the Advance tab, then settings, in the new window select the advanced tab, the click on change. Here, untick the \"Automatically manage paging file for all drives\" Select then the Custom size and in the initial size (MB) box. Enter 4096 and in the Maximum size (MB) box enter 7000 or 8000. (The maximum size can be increased if the mosaic to be created is very large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the neccesary libraries for downloading data \n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xmltodict\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will create a folder, where the notebook is located, and all\n",
    "the downloaded images will be stored there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input for \"test name\" eg 'PRD_STANDARDISED_TEST_WPS_S1x10_RUN1'\n",
    "RUNNAME = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fmt='%Y%m%dT%H%M%S'\n",
    "pretty_fmt='%Y-%m-%d %H:%M:%S'\n",
    "headers = {'Content-type': 'application/xml','User-Agent': 'curl/7.65.1'}\n",
    "\n",
    "wps_test_config = {\n",
    "        'gs:Download':{\n",
    "            'template_xml':'gsdownload-template.xml',\n",
    "            'mimetype':'application/zip',\n",
    "            'output':'result.zip',\n",
    "        }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it must be noted that this script in order to work, the input images must be of the same granule from different time stamps. This script will not create a mosaic of diferent granules but a mosaic of the same granule without any cloud coverage using diferent sensing dates of the same granule.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of granules from dowloaded csv \n",
    "#(Those are the resulting csv from eods-api-example-generate-cloudless-mosaic Notebook)\n",
    "col_list = [\"typename\"]\n",
    "min_cloud_per_granule = pd.read_csv(r\"C:\\Users\\.............\\min_cloud_per_granule.csv\")\n",
    "\n",
    "min_cloud_per_granule_per_orbit = pd.read_csv(r\"C:\\Users\\..............\\min-cloud-per-granule-per-orbit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USER INPUT\n",
    "### Enter your token in PRD_DM = \n",
    "\n",
    "wps_job_request_list = [\n",
    "     {'tool':'gs:Download','layer_list':min_cloud_per_granule_per_orbit[\"typename\"],'dl_bool':True}\n",
    "]\n",
    "\n",
    "PRD_DM = \"3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl\"\n",
    "ACCESS_TOKEN = PRD_DM\n",
    "\n",
    "ENV_PRD = 'https://earthobs.defra.gov.uk'\n",
    "wps_env = ENV_PRD\n",
    "\n",
    "wps_server = wps_env + '/geoserver/ows/?access_token=' + ACCESS_TOKEN + '&SERVICE=WPS&VERSION=1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# submitting the WPS request(s)\n",
    "\n",
    "def mod_the_xml(xml_name,layer_name):\n",
    "    \"\"\"\n",
    "    function read xml payload template and modify the payload with the config\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(Path.cwd() / 'xml' / xml_name,'r') as template_xml:\n",
    "        file_data = template_xml.read()\n",
    "    modded_xml = file_data.replace('template_layer_name', layer_name)\n",
    "    \n",
    "    return modded_xml\n",
    "\n",
    "def submit_wps_request(payload):\n",
    "    \"\"\"\n",
    "    function submit the wps POST request\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        wps_submit_response = requests.post(wps_server + '&REQUEST=EXECUTE', data=payload, headers=headers, verify=True)\n",
    "        print('\\n' + datetime.utcnow().isoformat() + ' :: request POST url =' + wps_server + '&REQUEST=EXECUTE')\n",
    "    except:\n",
    "        print('wps POST was not successful')\n",
    "    \n",
    "    # receive status code\n",
    "    timestamp_submission = datetime.utcnow()\n",
    "    status_code = wps_submit_response.status_code\n",
    "        \n",
    "    if status_code == 200 and not wps_submit_response.text.find('ExceptionReport') > 0:\n",
    "        execution_id = wps_submit_response.text.split('executionId=')[1].split('&')[0]        \n",
    "        print('\\n' + datetime.utcnow().isoformat() + ':: WPS job ' + execution_id + ' was successfully submitted')\n",
    "        print('\\n' + datetime.utcnow().isoformat() + ':: ' + str(wps_submit_response.content))\n",
    "        return execution_id\n",
    "    else:\n",
    "        print(datetime.utcnow().isoformat() + ' :: wps request was not successful :: STATUS = ' + str(status_code))\n",
    "        print(datetime.utcnow().isoformat() + ' :: response = ' + str(wps_submit_response.content))\n",
    "        return None\n",
    "\n",
    "execution_id_dict={}\n",
    "\n",
    "for item in wps_job_request_list:\n",
    "        \n",
    "    for lyr in item['layer_list']:\n",
    "        print('\\n' + datetime.utcnow().isoformat() + ' :: tool=' + item['tool'] + ', lyr=' + lyr + ', download=' + str(item['dl_bool']))\n",
    "        modded_xml = mod_the_xml(wps_test_config[item['tool']]['template_xml'],lyr)\n",
    "        execution_id = submit_wps_request(modded_xml)\n",
    "        execution_id_dict.update({execution_id:lyr})\n",
    "        submission_time = datetime.utcnow()\n",
    "\n",
    "print('\\n' + datetime.utcnow().isoformat() + ' :: A TOTAL OF ' + str(len(execution_id_dict)) +  ' WPS REQUESTS SUBMITTED')\n",
    "\n",
    "print('\\n' + datetime.utcnow().isoformat() + ' :: The ExecutionIDs are :: ' + str(execution_id_dict.keys()))\n",
    "\n",
    "print('\\n' + datetime.utcnow().isoformat() + ' :: WPS Check Executions URL is ' + wps_server + '&REQUEST=GetExecutions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poll the status page and wait until all jobs are completed\n",
    "\n",
    "# create output directory and log file\n",
    "Path(Path.cwd() / 'output').mkdir(parents=True, exist_ok=True)\n",
    "my_path = Path.cwd() / 'output' / str(submission_time.strftime(output_fmt) + '-' + RUNNAME)\n",
    "my_path.mkdir(parents=True, exist_ok=True)\n",
    "log_file_name = my_path / str(submission_time.strftime(output_fmt) + '-' + RUNNAME + '-wps-log.csv')\n",
    "poll_frequency_sec = 60\n",
    "frames = []\n",
    "\n",
    "# poll api for 1 hour unless all jobs submitted are NOT STATUS=RUNNING\n",
    "for i in range(1,3600):\n",
    "    \n",
    "    check_time = datetime.utcnow()\n",
    "    if i == 1:\n",
    "        print('\\n#' + format(i, '03') + ' of #n   :: ' + submission_time.isoformat() + ' :: Logging file: ' + str(log_file_name))\n",
    "    \n",
    "    print('\\n#' + format(i, '03') + ' of #n   :: ' + check_time.isoformat() + ' :: Poll Time')\n",
    "    \n",
    "    start_index = 0\n",
    "    matching_of_dicts = []\n",
    "    for page_counter in range(0,1000,10):\n",
    "\n",
    "        # constrcut the paging URL and make the request\n",
    "        all_status_url = wps_server + '&REQUEST=GetExecutions&startIndex=' + str(page_counter)\n",
    "\n",
    "        r = requests.get(all_status_url,headers=headers)\n",
    "\n",
    "        # parse the xml to an ordered dict using 3rd party imported module xmltodict\n",
    "        d = xmltodict.parse(r.content)\n",
    "\n",
    "        # grab the dicts of 'wps:ExecuteResponse' and add to a list\n",
    "        jobs_list = [value for value in d['wps:GetExecutionsResponse']['wps:ExecuteResponse']]\n",
    "\n",
    "        # add this to a master list\n",
    "        matching_of_dicts.append(jobs_list)\n",
    "\n",
    "        # if there is no 'next' attribute, then you're on the last xml page so break out the loop\n",
    "        if not any('@next' == key for key in list(d['wps:GetExecutionsResponse'].keys())):\n",
    "            break\n",
    "\n",
    "    num_of_xml_pages = int(page_counter / 10) + 1\n",
    "    duration_to_parse_xml_dt = datetime.utcnow() - check_time\n",
    "    duration_to_parse_xml_sec = round(duration_to_parse_xml_dt.total_seconds(),0)\n",
    "    print('#' + format(i, '03') + ' of #n   :: ' + datetime.utcnow().isoformat() + ' :: ' + str(num_of_xml_pages) + ' xml pages parsed in ' + str(duration_to_parse_xml_sec) + ' seconds')\n",
    "            \n",
    "    # now flatten the list into all responses\n",
    "    response_list = [item['wps:Status'] for sublist in matching_of_dicts for item in sublist]\n",
    "\n",
    "    # parse dict to pandas dataframe and set the index\n",
    "    df_temp = pd.DataFrame.from_dict(response_list)\n",
    "    df = df_temp.set_index('wps:JobID')\n",
    "\n",
    "    # filter df on current job_ids\n",
    "    filter_df  = df[df.index.isin(execution_id_dict.keys())]\n",
    "            \n",
    "    # add \"check time\" as column\n",
    "    filter_df.loc[:,'check_time'] = check_time.isoformat()\n",
    "    \n",
    "    # append on some extra info to the dataframe using the index\n",
    "    for index, row in filter_df.iterrows():\n",
    "        filter_df.loc[index,'layer_name'] = execution_id_dict[index]\n",
    "        filter_df.loc[index,'dl_url'] = wps_server + '&REQUEST=GetExecutionResult&EXECUTIONID='  + index + '&outputId=' + wps_test_config['gs:Download']['output'] + '&mimetype=' + wps_test_config['gs:Download']['mimetype']\n",
    "    \n",
    "    # sort out the indices and concatenate dataframes together and output to csv\n",
    "    filter_df.reset_index(inplace=True)\n",
    "    filter_df.set_index(['wps:JobID','check_time'],inplace=True)\n",
    "    frames.append(filter_df)\n",
    "    rolling_merged_df = pd.concat(frames)\n",
    "    rolling_merged_df.to_csv(log_file_name)\n",
    "        \n",
    "    # if NO running processes, then break, otherwise loop again\n",
    "    if not any(filter_df['wps:Status'] == 'RUNNING'):\n",
    "        break\n",
    "    print('#' + format(i, '03') + ' of #n   :: ' + datetime.utcnow().isoformat() + ' :: Jobs are still \"STATUS=RUNNING\"...script will poll again in ' + str(poll_frequency_sec) + 'sec time ...')\n",
    "    \n",
    "    time.sleep(poll_frequency_sec)\n",
    "    \n",
    "print('\\n#' + format(i, '03') + ' of #' + format(i, '03') + ' :: ' + datetime.utcnow().isoformat() + ' :: Script has finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export summary wps jobs with \"runtimes\"\n",
    "sumdf = rolling_merged_df.reset_index()\n",
    "grouped = sumdf.groupby(['wps:JobID','wps:Status']).first()\n",
    "grouped.to_csv(my_path / str(submission_time.strftime(output_fmt) + '-' + RUNNAME + '-summary-wps-log.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD, WRITE FILE AND RENAME FILE\n",
    "for index, row in filter_df.iterrows():\n",
    "    if row['wps:Status'] == 'SUCCEEDED':\n",
    "        print('\\n\\n' + datetime.utcnow().isoformat() + ' :: DOWNLOAD START :: URL = ' + row['dl_url'])        \n",
    "        response = requests.get(row['dl_url'],headers=headers);\n",
    "        print(datetime.utcnow().isoformat() + ' :: DOWNLOAD COMPLETE')\n",
    "        local_file_name = my_path / str(row['layer_name'].split(':')[-1] + '.zip')\n",
    "        print(datetime.utcnow().isoformat() + ' :: FILE WRITE START :: FILE = ' + str(local_file_name))        \n",
    "        with open(local_file_name, 'wb') as f:\n",
    "            f.write(response.content);\n",
    "            f.close();        \n",
    "        print(datetime.utcnow().isoformat() + ' :: FILE WRITE COMPLETE')\n",
    "        print(datetime.utcnow().isoformat() + ' :: EXTRACT ARCHIVE FILE STARTED')\n",
    "                \n",
    "        # TODO. Check if downloaded file is actually a zip file type. Handle exception for an xml error report?\n",
    "        zip_ref = ZipFile(local_file_name, 'r')\n",
    "        zip_ref.extractall(my_path)\n",
    "        zip_ref.close()\n",
    "        print(datetime.utcnow().isoformat() + ' :: EXTRACT ARCHIVE FILE COMPLETE')\n",
    "        print(datetime.utcnow().isoformat() + ' :: RENAME/CLEAN UP DIRECTORY STARTED')\n",
    "        for f in zip_ref.filelist:\n",
    "            if f.filename.endswith(\".sld\"):\n",
    "                Path(my_path / f.filename).unlink()\n",
    "            \n",
    "            # rename the uuid.tiff to layer_name.tif\n",
    "            elif f.filename.endswith(\".tif\") or f.filename.endswith(\".tiff\"):\n",
    "                Path(my_path / f.filename).rename(my_path / str(row['layer_name'].split(':')[-1] + '.tif'))\n",
    "            else:\n",
    "                raise ValueError(\"ERROR - some unknown file has been given:\", f)\n",
    "        \n",
    "        # remove the zip file from disk\n",
    "        Path(local_file_name).unlink()\n",
    "        print(datetime.utcnow().isoformat() + ' :: RENAME/CLEAN UP DIRECTORY COMPLETE')\n",
    "\n",
    "print('\\n\\n' + datetime.utcnow().isoformat() + ' :: #### SCRIPT FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries for Mosaic creation\n",
    "import rsgislib\n",
    "import rsgislib.imageutils\n",
    "import rsgislib.rastergis\n",
    "import rsgislib.imagecalc\n",
    "import rsgislib.imagecalc.calcindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the folder and the tif files for the mosaic\n",
    "dirpath= my_path\n",
    "search_critiria = \"*.tif\"\n",
    "q = os.path.join(dirpath,search_critiria)\n",
    "S2_fps = glob.glob(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing libraries for converting .tif to .kea for the Mosaic creation\n",
    "import os.path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the convertion \n",
    "def replaceGTIFF_kea(inputtext):\n",
    "    outputtext = \"\"    \n",
    "    for w in inputtext:\n",
    "        w = w.replace(\"GEOTIFF\",\"KEA\")\n",
    "        w = w.replace(\".TIF\",\".kea\")\n",
    "        # this line should be unnecessary since Landsat MTL files use capital letters       \n",
    "        # but just in case you have one that doesn't\n",
    "        w = w.replace(\".tif\",\".kea\")\n",
    "        outputtext += w\n",
    "    return outputtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all *.TIF files and *MTL.txt files in the current directory\n",
    "directory = os.chdir(my_path)\n",
    "dirFileList = os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dirFileList\n",
    "tifFileList = [f for f in dirFileList if ((f[-4:]=='.TIF')or(f[-4:]=='.tif'))]\n",
    "MTLFileList = [f for f in dirFileList if (f[-7:]=='MTL.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output format (GDAL code)\n",
    "outFormat = 'KEA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gdal_translate on all TIFs to convert to KEA\n",
    "for t in tifFileList:\n",
    "    gdaltranscmd = \"gdal_translate -of \"+outFormat+\" \"+t+\" \"+t[:-4]+\".kea\"\n",
    "    print (gdaltranscmd)\n",
    "    os.system(gdaltranscmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new header file referring to .kea files rather than .TIF\n",
    "for m in MTLFileList:\n",
    "    inputtext = file(m).readlines()\n",
    "    outputtext = replaceGTIFF_kea(inputtext)\n",
    "    outputfilebase = m[:-4]\n",
    "    outputfile = outputfilebase + \"_kea.txt\"\n",
    "    out = file(outputfile,\"w\")\n",
    "    out.write(outputtext)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the paths for finding the created .kea files\n",
    "dirpath= my_path\n",
    "search_critiria_kea = \"*.kea\"\n",
    "q = os.path.join(dirpath,search_critiria_kea)\n",
    "S2 = glob.glob(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mosaicing all the .kea files \n",
    "inImages = S2\n",
    "refImg = FG_Sentinel_ROI.kea\n",
    "outCompImg = CompSentinel.kea\n",
    "outMskImg = Mask.kea\n",
    "\n",
    "rsgislib.imageutils.imagecomp.createMaxNDVINDWIComposite(refImg, inImages, 3, 8, 9, outRefImg, outCompImg, outMskImg, tmpPath='./tmp', gdalformat='KEA', dataType=None, calcStats=True, reprojmethod='cubic', use_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the created .kea mosaic back to .tif \n",
    "def replaceGTIFF_kea(inputtext):\n",
    "    outputtext = \"\"    \n",
    "    for w in inputtext:\n",
    "        w = w.replace(\"KEA\",\"GEOTIFF\")\n",
    "        w = w.replace(\".kea\",\".TIF\")\n",
    "        # this line should be unnecessary since Landsat MTL files use capital letters       \n",
    "        # but just in case you have one that doesn't\n",
    "        w = w.replace(\".kea\",\".tif\")\n",
    "        outputtext += w\n",
    "    return outputtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all *.kea files and *MTL.txt files in the current directory\n",
    "directory = os.chdir(\"C:/Users/Kostas.Sideris/output/Mosaic\")\n",
    "dirFileList = os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dirFileList\n",
    "tifFileList = [f for f in dirFileList if ((f[-4:]=='.KEA')or(f[-4:]=='.kea'))]\n",
    "MTLFileList = [f for f in dirFileList if (f[-7:]=='MTL.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output format (GDAL code)\n",
    "outFormat = 'GTiff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run gdal_translate on the .KEA Mosaic  to convert to .tiff\n",
    "for t in tifFileList:\n",
    "    gdaltranscmd = \"gdal_translate -of \"+outFormat+\" \"+t+\" \"+t[:-4]+\".tif\"\n",
    "    print (gdaltranscmd)\n",
    "    os.system(gdaltranscmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
